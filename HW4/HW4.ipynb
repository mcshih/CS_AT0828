{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"x_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "x_test = np.load(\"x_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550, 300)\n"
     ]
    }
   ],
   "source": [
    "# 550 data with 300 features\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# It's a binary classification problem \n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "K-fold data partition: Implement the K-fold cross-validation function. Your function should take K as an argument and return a list of lists (len(list) should equal to K), which contains K elements. Each element is a list contains two parts, the first part contains the index of all training folds, e.g. Fold 2 to Fold 5 in split 1. The second part contains the index of validation fold, e.g. Fold 1 in  split 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(x_train, y_train, k=5):\n",
    "    length = len(x_train)\n",
    "    idx_list = list(range(length))\n",
    "    val_data = np.split(np.array(idx_list), k)\n",
    "    final_data = []\n",
    "    for fold in range(k):\n",
    "        list_train = []\n",
    "        list_val = []\n",
    "        for index in range(length):\n",
    "            if index in val_data[fold]:\n",
    "                list_val.append((x_train[index], y_train[index]))\n",
    "            else:\n",
    "                list_train.append((x_train[index], y_train[index]))\n",
    "        final_data.append([np.array(list_train),np.array(list_val)])\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16637/4119303063.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  final_data.append([np.array(list_train),np.array(list_val)])\n"
     ]
    }
   ],
   "source": [
    "kfold_data = cross_validation(x_train, y_train, k=10)\n",
    "assert len(kfold_data) == 10 # should contain 10 fold of data\n",
    "assert len(kfold_data[0]) == 2 # each element should contain train fold and validation fold\n",
    "assert kfold_data[0][1].shape[0] == 55 # The number of data in each validation fold should equal to training data divieded by K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 1, Training index: [ 0  1  4  5  6  7  8  9 10 13 14 15 16 17 18 19], Validation index: [ 2  3 11 12]\n",
      "Split: 2, Training index: [ 0  1  2  3  6  7  8  9 10 11 12 13 14 15 16 17], Validation index: [ 4  5 18 19]\n",
      "Split: 3, Training index: [ 0  2  3  4  5  7  8  9 10 11 12 14 16 17 18 19], Validation index: [ 1  6 13 15]\n",
      "Split: 4, Training index: [ 0  1  2  3  4  5  6 10 11 12 13 14 15 17 18 19], Validation index: [ 7  8  9 16]\n",
      "Split: 5, Training index: [ 1  2  3  4  5  6  7  8  9 11 12 13 15 16 18 19], Validation index: [ 0 10 14 17]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.arange(20)\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kfold_data= []\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    print(\"Split: %s, Training index: %s, Validation index: %s\" % (i+1, train_index, val_index))\n",
    "    kfold_data.append([train_index, val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(kfold_data) == 5 # should contain 5 fold of data\n",
    "assert len(kfold_data[0]) == 2 # each element should contains index of training fold and validation fold\n",
    "assert kfold_data[0][1].shape[0] == 4 # The number of data in each validation fold should equal to training data divieded by K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Using sklearn.svm.SVC to train a classifier on the provided train set and conduct the grid search of “C” and “gamma” to find the best parameters by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, kernel='rbf', gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.68909091 0.68909091 0.68909091 0.68909091 0.68909091 0.68909091\n",
      "  0.88       0.88727273]\n",
      " [0.68909091 0.68909091 0.68909091 0.68909091 0.68909091 0.69090909\n",
      "  0.88545455 0.87272727]\n",
      " [0.68909091 0.68909091 0.68909091 0.68909091 0.68909091 0.68909091\n",
      "  0.88545455 0.87090909]\n",
      " [0.68909091 0.68909091 0.68909091 0.68909091 0.68909091 0.69090909\n",
      "  0.89818182 0.71636364]\n",
      " [0.68909091 0.68909091 0.68909091 0.68909091 0.68909091 0.68909091\n",
      "  0.68909091 0.68909091]\n",
      " [0.68909091 0.68909091 0.68909091 0.68909091 0.68909091 0.68909091\n",
      "  0.68909091 0.68909091]\n",
      " [0.68909091 0.68909091 0.68909091 0.68909091 0.68909091 0.68909091\n",
      "  0.68909091 0.68909091]]\n"
     ]
    }
   ],
   "source": [
    "## your code\n",
    "grid_C = [10000., 1000., 100., 10., 1, 0.1, 0.01]\n",
    "grid_gamma = [1000., 100., 10., 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "all_mean_score = np.zeros((len(grid_C), len(grid_gamma)))\n",
    "\n",
    "for X, para_C in enumerate(grid_C):\n",
    "    for Y, para_gamma in enumerate(grid_gamma):\n",
    "        #print(\"grid: C: %s, gamma: %s\" % (para_C, para_gamma))\n",
    "\n",
    "        K = 5\n",
    "        kf = KFold(n_splits=K, shuffle=True)\n",
    "        score_list = []\n",
    "        for i, (train_index, val_index) in enumerate(kf.split(x_train)):\n",
    "            x_train_part = []\n",
    "            y_train_part = []\n",
    "            x_val_part = []\n",
    "            y_val_part = []\n",
    "            for index in range(len(x_train)):\n",
    "                if index in train_index:\n",
    "                    x_train_part.append(x_train[index])\n",
    "                    y_train_part.append(y_train[index])\n",
    "                else:\n",
    "                    x_val_part.append(x_train[index])\n",
    "                    y_val_part.append(y_train[index])\n",
    "            \n",
    "            clf = SVC(C=para_C, kernel='rbf', gamma=para_gamma).fit(x_train_part, y_train_part)\n",
    "            score = clf.score(x_val_part, y_val_part)\n",
    "            #print(score, end=\"\\t\")\n",
    "            score_list.append(score)    \n",
    "        all_mean_score[X][Y] = np.mean(score_list)\n",
    "\n",
    "print(all_mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.0, 0.001)\n"
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "best_parameters = (-1, -1)\n",
    "for X in range(len(grid_C)):\n",
    "    for Y in range(len(grid_gamma)):\n",
    "        if all_mean_score[X][Y] > best_score:\n",
    "            best_parameters = (grid_C[X], grid_gamma[Y])\n",
    "            best_score = all_mean_score[X][Y]\n",
    "\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the grid search results of your SVM. The x, y represents the hyperparameters of “gamma” and “C”, respectively. And the color represents the average score of validation folds\n",
    "You reults should be look like this reference image below ![image](https://miro.medium.com/max/1296/1*wGWTup9r4cVytB5MOnsjdQ.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(all_mean_score, cmap='gist_rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Train your SVM model by the best parameters you found from question 2 on the whole training set and evaluate the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.8958333333333334\n"
     ]
    }
   ],
   "source": [
    "best_para_C, best_para_gamma = best_parameters\n",
    "\n",
    "best_model = SVC(C=best_para_C, kernel='rbf', gamma=best_para_gamma).fit(x_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(x_test)\n",
    "print(\"Accuracy score: \", accuracy_score(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb38cdc93567307ab4accfb59d38a4ec7f33df93973f81a77b6fa6936a94a054"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('doctr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
